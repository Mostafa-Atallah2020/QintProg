{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(f\"./../\")\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict\n",
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "\n",
    "from src.qaoa_scheduling import QAOACircuit, QAOAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAOAAnalyzer:\n",
    "    def __init__(self, n_vertices: int, g6_path: str, random_seed: int = None):\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "        self.n_vertices = n_vertices\n",
    "        self.g6_path = g6_path\n",
    "        self.results_dir = Path(\"computed_results_greedy\")\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    def read_g6_graphs(self) -> List[nx.Graph]:\n",
    "        \"\"\"Read graphs from a g6 format file.\"\"\"\n",
    "        with open(self.g6_path, \"rb\") as f:\n",
    "            graphs = list(nx.read_graph6(f))\n",
    "        return graphs\n",
    "\n",
    "    def analyze_graph(self, G: nx.Graph) -> Dict:\n",
    "        \"\"\"Analyze a single graph for QAOA scheduling metrics.\"\"\"\n",
    "        # Generate random gamma values in (0, 2Ï€]\n",
    "        gamma_gates = {\n",
    "            (min(u, v), max(u, v)): np.random.uniform(low=0.0, high=2 * np.pi, size=1)[0]\n",
    "            for u, v in G.edges()\n",
    "        }\n",
    "        circuit = QAOACircuit(\n",
    "            n_qubits=G.number_of_nodes(), gamma_gates=gamma_gates, beta_time=1.0\n",
    "        )\n",
    "\n",
    "        # Create scheduler and get results\n",
    "        scheduler = QAOAScheduler(circuit)\n",
    "        lp_result = scheduler.solve_lp()\n",
    "        greedy_result = scheduler.schedule_greedy()\n",
    "\n",
    "        seqt = lp_result.total_time_before\n",
    "        greedyt = greedy_result.total_time_after\n",
    "        lpt = lp_result.total_time_after\n",
    "\n",
    "        # Calculate improvement\n",
    "        improvement_percentage = np.abs(lpt - greedyt) / greedyt\n",
    "\n",
    "        return {\"edges\": G.number_of_edges(), \"improvement\": improvement_percentage}\n",
    "\n",
    "    def get_result_path(self):\n",
    "        \"\"\"Get path for saving/loading results.\"\"\"\n",
    "        filename = f\"results_greedy_v{self.n_vertices}.json\"\n",
    "        return self.results_dir / filename\n",
    "\n",
    "    def save_results(self, edge_counts, means, stds):\n",
    "        \"\"\"Save aggregated results.\"\"\"\n",
    "        results = {\n",
    "            \"n_vertices\": self.n_vertices,\n",
    "            \"edge_counts\": edge_counts,\n",
    "            \"means\": means,\n",
    "            \"stds\": stds,\n",
    "        }\n",
    "        with open(self.get_result_path(), \"w\") as f:\n",
    "            # Convert numpy arrays to lists for JSON serialization\n",
    "            results = {\n",
    "                k: v.tolist() if isinstance(v, np.ndarray) else v\n",
    "                for k, v in results.items()\n",
    "            }\n",
    "            json.dump(results, f)\n",
    "\n",
    "    def load_results(self):\n",
    "        \"\"\"Load results from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(self.get_result_path(), \"r\") as f:\n",
    "                results = json.load(f)\n",
    "                return (\n",
    "                    np.array(results[\"edge_counts\"]),\n",
    "                    np.array(results[\"means\"]),\n",
    "                    np.array(results[\"stds\"]),\n",
    "                )\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Results file for vertices={self.n_vertices} not found\")\n",
    "            return None, None, None\n",
    "\n",
    "    def process_all_graphs(self, run_id) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Process all graphs and compute statistics for a single run.\"\"\"\n",
    "        graphs = self.read_g6_graphs()\n",
    "\n",
    "        # Group results by edge count as we process\n",
    "        edge_groups = defaultdict(list)\n",
    "\n",
    "        for i, G in enumerate(graphs):\n",
    "            result = self.analyze_graph(G)\n",
    "            edge_groups[result[\"edges\"]].append(result[\"improvement\"])\n",
    "\n",
    "            # No intermediate progress reporting\n",
    "\n",
    "        # Final calculation and save\n",
    "        edge_counts = np.array(sorted(edge_groups.keys()))\n",
    "        means = np.array([np.mean(edge_groups[ec]) for ec in edge_counts])\n",
    "\n",
    "        # Save results for this run\n",
    "        self.save_results(edge_counts, means, run_id)\n",
    "        \n",
    "        return edge_counts, means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_save_graphs(input_path: str, output_path: str, n_samples: int = 1000):\n",
    "    \"\"\"Sample graphs from g6 file and save to new file if count exceeds n_samples.\"\"\"\n",
    "    # Read all graphs\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        graphs = list(nx.read_graph6(f))\n",
    "\n",
    "    total_graphs = len(graphs)\n",
    "    if total_graphs > n_samples:\n",
    "        # Randomly sample n_samples graphs\n",
    "        sampled_graphs = random.sample(graphs, n_samples)\n",
    "\n",
    "        # Save sampled graphs\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            for G in sampled_graphs:\n",
    "                f.write(nx.to_graph6_bytes(G, header=False))\n",
    "\n",
    "        print(f\"Sampled {n_samples} graphs from {total_graphs} total graphs\")\n",
    "        return output_path\n",
    "    else:\n",
    "        print(f\"Using all {total_graphs} graphs (less than {n_samples})\")\n",
    "        return input_path\n",
    "\n",
    "\n",
    "def compute_all_results(vertex_range=(3, 6), n_samples: int = 1000, n_runs: int = 5):\n",
    "    \"\"\"Compute and save results for all vertex counts with multiple runs for statistical significance.\"\"\"\n",
    "    # Create output directories\n",
    "    sampled_dir = Path(\"sampled_graphs\")\n",
    "    sampled_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Process each vertex count\n",
    "    for n_vertices in range(vertex_range[0], vertex_range[1] + 1):\n",
    "        print(f\"\\nProcessing {n_vertices} vertices...\")\n",
    "\n",
    "        # Handle original and sampled file paths\n",
    "        original_path = f\"./data/graphs/graph{n_vertices}c.g6\"\n",
    "        sampled_path = sampled_dir / f\"sampled_graph{n_vertices}c.g6\"\n",
    "\n",
    "        # Sample graphs if needed and get path to use\n",
    "        g6_path = sample_and_save_graphs(original_path, sampled_path, n_samples)\n",
    "\n",
    "        # Create analyzer\n",
    "        analyzer = QAOAAnalyzer(n_vertices, g6_path)\n",
    "        \n",
    "        # Run multiple times with different random seeds\n",
    "        all_improvements = defaultdict(list)\n",
    "        \n",
    "        for run_id in range(n_runs):\n",
    "            # Set different random seed for each run\n",
    "            np.random.seed(42 + run_id)\n",
    "            print(f\"\\nRun {run_id+1}/{n_runs} for {n_vertices} vertices...\")\n",
    "            \n",
    "            # Process graphs for this run\n",
    "            graphs = analyzer.read_g6_graphs()\n",
    "            edge_improvements = defaultdict(list)\n",
    "            \n",
    "            for G in graphs:\n",
    "                result = analyzer.analyze_graph(G)\n",
    "                edge_improvements[result[\"edges\"]].append(result[\"improvement\"])\n",
    "            \n",
    "            # Compute means for this run\n",
    "            edge_counts = sorted(edge_improvements.keys())\n",
    "            run_means = [np.mean(edge_improvements[ec]) for ec in edge_counts]\n",
    "            \n",
    "            # Collect results from this run\n",
    "            for ec, imp in zip(edge_counts, run_means):\n",
    "                all_improvements[ec].append(imp)\n",
    "        \n",
    "        # Aggregate results across runs\n",
    "        unique_edge_counts = sorted(all_improvements.keys())\n",
    "        means = np.array([np.mean(all_improvements[ec]) for ec in unique_edge_counts])\n",
    "        stds = np.array([np.std(all_improvements[ec], ddof=1) for ec in unique_edge_counts])\n",
    "        \n",
    "        # Save only the final aggregated results\n",
    "        analyzer.save_results(np.array(unique_edge_counts), means, stds)\n",
    "        \n",
    "        print(f\"Completed all runs for {n_vertices} vertices\")\n",
    "\n",
    "\n",
    "def plot_from_saved_results(vertex_range=(3, 6), n_samples: int = 1000):\n",
    "    \"\"\"Create plot using saved aggregate results.\"\"\"\n",
    "    # Set publication-quality style\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"figure.figsize\": (10, 6),\n",
    "            \"figure.dpi\": 300,\n",
    "            \"font.size\": 16,\n",
    "            \"font.family\": \"serif\",\n",
    "            \"axes.labelsize\": 18,\n",
    "            \"axes.titlesize\": 20,\n",
    "            \"xtick.labelsize\": 16,\n",
    "            \"ytick.labelsize\": 16,\n",
    "            \"legend.fontsize\": 14,\n",
    "            \"figure.facecolor\": \"white\",\n",
    "            \"axes.facecolor\": \"white\",\n",
    "            \"axes.grid\": True,\n",
    "            \"grid.color\": \"#E5E5E5\",\n",
    "            \"grid.linewidth\": 1.5,\n",
    "            \"lines.linewidth\": 3,\n",
    "            \"axes.linewidth\": 2,\n",
    "            \"xtick.major.width\": 2,\n",
    "            \"ytick.major.width\": 2,\n",
    "            \"xtick.major.size\": 10,\n",
    "            \"ytick.major.size\": 10,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Color map for different vertex counts\n",
    "    colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \"#8c564b\"]\n",
    "    markers = [\"o\", \"s\", \"^\", \"D\", \"v\", \"<\"]\n",
    "\n",
    "    # Plot for each vertex count\n",
    "    for i, n_vertices in enumerate(range(vertex_range[0], vertex_range[1] + 1)):\n",
    "        # Load saved aggregate results\n",
    "        analyzer = QAOAAnalyzer(n_vertices, \"\")  # path not needed for loading\n",
    "        edge_counts, means, stds = analyzer.load_results()\n",
    "        \n",
    "        if edge_counts is None:\n",
    "            print(f\"No results found for {n_vertices} vertices, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Plot with unique color and marker\n",
    "        ax.errorbar(\n",
    "            edge_counts,\n",
    "            means * 100,  # Convert to percentage\n",
    "            yerr=stds * 100,  # SEM from multiple runs\n",
    "            fmt=f\"{markers[i]}-\",\n",
    "            capsize=4,\n",
    "            capthick=1.5,\n",
    "            elinewidth=1.5,\n",
    "            markersize=8,\n",
    "            color=colors[i],\n",
    "            label=f\"{n_vertices} Vertices\",\n",
    "            markeredgewidth=1.5,\n",
    "            markeredgecolor=\"black\",\n",
    "        )\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_xlabel(\"Number of Edges\", labelpad=10)\n",
    "    ax.set_ylabel(\"LP vs Greedy Improvement (%)\", labelpad=10)\n",
    "    ax.set_title(\"QAOA Circuit Scheduling Comparison\", pad=15)\n",
    "    \n",
    "    # Ensure y-axis doesn't show negative values for percentage improvements\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.7, linewidth=1.5)\n",
    "    ax.margins(x=0.05)\n",
    "\n",
    "    # Customize legend\n",
    "    ax.legend(\n",
    "        frameon=True,\n",
    "        edgecolor=\"black\",\n",
    "        fancybox=False,\n",
    "        loc=\"best\",\n",
    "        ncol=2,\n",
    "        columnspacing=1,\n",
    "        handletextpad=0.5,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    output_dir = Path(\"plots\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    plt.savefig(\n",
    "        output_dir\n",
    "        / f\"greedy_lp_improvement_v{vertex_range[0]}-{vertex_range[1]}_sampled{n_samples}.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        dpi=300,\n",
    "        facecolor=\"white\",\n",
    "    )\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 3 vertices...\n",
      "Using all 2 graphs (less than 1000)\n",
      "\n",
      "Run 1/10 for 3 vertices...\n",
      "\n",
      "Run 2/10 for 3 vertices...\n",
      "\n",
      "Run 3/10 for 3 vertices...\n",
      "\n",
      "Run 4/10 for 3 vertices...\n",
      "\n",
      "Run 5/10 for 3 vertices...\n",
      "\n",
      "Run 6/10 for 3 vertices...\n",
      "\n",
      "Run 7/10 for 3 vertices...\n",
      "\n",
      "Run 8/10 for 3 vertices...\n",
      "\n",
      "Run 9/10 for 3 vertices...\n",
      "\n",
      "Run 10/10 for 3 vertices...\n",
      "Completed all runs for 3 vertices\n",
      "\n",
      "Processing 4 vertices...\n",
      "Using all 6 graphs (less than 1000)\n",
      "\n",
      "Run 1/10 for 4 vertices...\n",
      "\n",
      "Run 2/10 for 4 vertices...\n",
      "\n",
      "Run 3/10 for 4 vertices...\n",
      "\n",
      "Run 4/10 for 4 vertices...\n",
      "\n",
      "Run 5/10 for 4 vertices...\n",
      "\n",
      "Run 6/10 for 4 vertices...\n",
      "\n",
      "Run 7/10 for 4 vertices...\n",
      "\n",
      "Run 8/10 for 4 vertices...\n",
      "\n",
      "Run 9/10 for 4 vertices...\n",
      "\n",
      "Run 10/10 for 4 vertices...\n",
      "Completed all runs for 4 vertices\n",
      "\n",
      "Processing 5 vertices...\n",
      "Using all 21 graphs (less than 1000)\n",
      "\n",
      "Run 1/10 for 5 vertices...\n",
      "\n",
      "Run 2/10 for 5 vertices...\n",
      "\n",
      "Run 3/10 for 5 vertices...\n",
      "\n",
      "Run 4/10 for 5 vertices...\n",
      "\n",
      "Run 5/10 for 5 vertices...\n",
      "\n",
      "Run 6/10 for 5 vertices...\n",
      "\n",
      "Run 7/10 for 5 vertices...\n",
      "\n",
      "Run 8/10 for 5 vertices...\n",
      "\n",
      "Run 9/10 for 5 vertices...\n",
      "\n",
      "Run 10/10 for 5 vertices...\n",
      "Completed all runs for 5 vertices\n",
      "\n",
      "Processing 6 vertices...\n",
      "Using all 112 graphs (less than 1000)\n",
      "\n",
      "Run 1/10 for 6 vertices...\n",
      "\n",
      "Run 2/10 for 6 vertices...\n",
      "\n",
      "Run 3/10 for 6 vertices...\n",
      "\n",
      "Run 4/10 for 6 vertices...\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10  # Number of runs for statistical significance\n",
    "compute_all_results(vertex_range=(3, 6), n_samples=1000, n_runs=num_runs)\n",
    "plot_from_saved_results(vertex_range=(3, 6), n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
